# 快速修复 llama 依赖问题

## 问题根源

`llama.cpp` 官方仓库不支持 Swift Package Manager（没有 Package.swift 文件），所以无法通过 SPM 添加。

## 最快解决方案：使用支持 SPM 的第三方封装

### 步骤 1：删除现有的错误 Package

在 Xcode 中：
1. 点击项目文件（蓝色图标）
2. 选择 `Package Dependencies` 标签
3. 删除 `llama.cpp`（如果存在）

### 步骤 2：添加正确的 Package

使用这个支持 iOS 15+ 的库：

**在 Xcode 中操作**：
```
File → Add Package Dependencies...
URL: https://github.com/ShenghaiWang/SwiftLlama
Branch: main
```

选择产品：勾选 `SwiftLlama`

### 步骤 3：修改代码

只需要修改两个文件：

#### 1. 修改 LibLlama.swift

将第一行：
```swift
import llama
```

改为：
```swift
import SwiftLlama
```

然后修改 `create_context` 方法，使用 SwiftLlama 的 API：
```swift
static func create_context(path: String) throws -> SwiftLlama {
    let swiftLlama = try SwiftLlama(modelPath: path)
    return swiftLlama
}
```

#### 2. 修改 LLMService.swift

```swift
import Foundation
import SwiftLlama

@MainActor
class LLMService: ObservableObject {
    @Published var state: LLMServiceState = .idle
    @Published var loadProgress: Double = 0.0
    
    private var swiftLlama: SwiftLlama?
    private var isGenerating = false
    
    func loadModel(path: String) async throws {
        state = .loading
        loadProgress = 0.0
        
        swiftLlama = try await Task.detached(priority: .userInitiated) {
            try SwiftLlama(modelPath: path)
        }.value
        
        state = .ready
        loadProgress = 1.0
    }
    
    func generateResponse(for prompt: String) -> AsyncStream<String> {
        return AsyncStream { continuation in
            Task {
                guard let swiftLlama = self.swiftLlama else {
                    continuation.yield("错误：模型未加载")
                    continuation.finish()
                    return
                }
                
                await MainActor.run {
                    state = .generating
                }
                
                // 使用 SwiftLlama 的流式 API
                for try await token in await swiftLlama.start(for: prompt) {
                    await MainActor.run {
                        continuation.yield(token)
                    }
                }
                
                await MainActor.run {
                    continuation.finish()
                    state = .ready
                }
            }
        }
    }
    
    func cancelGeneration() {
        isGenerating = false
        if state == .generating {
            state = .ready
        }
    }
}
```

---

## 但是！SwiftLlama 要求 iOS 18+

如果你的项目要求 iOS 15+，我们需要另一个方案。

## 备选方案：使用 llama.cpp 的官方 SwiftUI 示例代码

llama.cpp 官方有 iOS 示例，我们直接复用它的代码。

### 核心思路

1. 不使用 SPM
2. 直接编译 llama.cpp 为静态库
3. 通过 Bridging Header 调用 C API
4. 保留我们现有的 Swift 封装代码

### 实施步骤

#### 步骤 1：编译 llama.cpp

在终端执行：

```bash
cd ~/Downloads

# 如果还没有克隆
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp

# 编译 Metal 版本（支持 GPU 加速）
make LLAMA_METAL=1

# 生成的库文件：
# - libllama.a
# - libggml.a (在 ggml/ 目录下)
```

#### 步骤 2：复制库文件和头文件到项目

```bash
cd /Users/yinlu/Documents/mycode/ChatMate

# 创建 libs 目录
mkdir -p libs
mkdir -p include

# 复制库文件
cp ~/Downloads/llama.cpp/libllama.a libs/
cp ~/Downloads/llama.cpp/ggml/libggml.a libs/

# 复制头文件
cp ~/Downloads/llama.cpp/include/llama.h include/
cp ~/Downloads/llama.cpp/ggml/include/ggml.h include/
cp ~/Downloads/llama.cpp/ggml/include/ggml-metal.h include/
cp ~/Downloads/llama.cpp/ggml/include/ggml-alloc.h include/
cp ~/Downloads/llama.cpp/ggml/include/ggml-backend.h include/
```

#### 步骤 3：在 Xcode 中配置

1. **创建 Bridging Header**

在 Xcode 中创建文件：`ChatMate/ChatMate/ChatMate-Bridging-Header.h`

```objc
#ifndef ChatMate_Bridging_Header_h
#define ChatMate_Bridging_Header_h

#include "llama.h"
#include "ggml.h"

#endif
```

2. **添加库文件到项目**

在 Xcode 中：
- 将 `libs/` 文件夹拖入项目
- 确保勾选 "Copy items if needed"
- Target → Build Phases → Link Binary With Libraries
- 添加：
  - `libllama.a`
  - `libggml.a`
  - `Metal.framework`
  - `MetalKit.framework`

3. **配置搜索路径**

Target → Build Settings：

- **Header Search Paths**:
  ```
  $(PROJECT_DIR)/../include
  ```

- **Library Search Paths**:
  ```
  $(PROJECT_DIR)/../libs
  ```

- **Other Linker Flags**:
  ```
  -lc++
  ```

- **Swift Compiler - General**:
  - Objective-C Bridging Header: `ChatMate/ChatMate-Bridging-Header.h`

4. **修改 LibLlama.swift**

**移除第一行的 `import llama`**，因为通过 Bridging Header，C 函数已经自动暴露给 Swift。

所有的 `llama_xxx` 函数可以直接使用！

---

## 我推荐的最终方案

由于配置较复杂，我建议：

**直接使用 llama.cpp 官方的 SwiftUI 示例项目**

```bash
cd ~/Downloads/llama.cpp/examples/llama.swiftui

# 用 Xcode 打开
open llama.swiftui.xcodeproj
```

这个项目已经完整配置好了，可以直接运行。

然后我们：
1. 复制它的 `llama.cpp.swift` 文件夹到我们的项目
2. 复制它的编译配置
3. 替换我们的 LLMService

这样最快速、最稳定！

---

## 你想选择哪个方案？

1. **方案 A**：升级项目到 iOS 18+，使用 SwiftLlama（最简单）
2. **方案 B**：手动编译和集成 llama.cpp（最灵活）
3. **方案 C**：直接复用官方示例项目的配置（最快速）

告诉我你的选择，我来帮你实施！
